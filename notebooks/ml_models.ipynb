{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ae0db1-89d3-4468-a5d3-dc67318a7628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STL Model - Original Features\n",
      "                           Model  R² Train   R² Test      MAE     RMSE\n",
      "               Linear Regression  0.004784 -0.000149 0.195327 6.151465\n",
      "Polynomial Regression (Degree 2)  0.008164 -0.000022 0.196357 6.151076\n",
      "                   Decision Tree  0.679205  0.117265 0.187506 5.779117\n",
      "                   Random Forest  0.642711  0.085215 0.184914 5.883094\n",
      "\n",
      " HP Model - Original Features\n",
      "                           Model  R² Train   R² Test      MAE     RMSE\n",
      "               Linear Regression  0.003212 -0.000053 0.228711 5.076118\n",
      "Polynomial Regression (Degree 2)  0.016187  0.000029 0.238992 5.075910\n",
      "                   Decision Tree  0.505896  0.105657 0.241495 4.800343\n",
      "                   Random Forest  0.484905  0.068866 0.233817 4.898086\n",
      "\n",
      " STL Model - Engineered Features\n",
      "                           Model  R² Train  R² Test          MAE         RMSE\n",
      "               Linear Regression  1.000000 1.000000 2.384950e-16 3.809516e-15\n",
      "Polynomial Regression (Degree 2)  0.999988 0.968916 2.456012e-02 1.084456e+00\n",
      "                   Decision Tree  1.000000 0.118093 1.303212e-01 5.776405e+00\n",
      "                   Random Forest  0.983524 0.097096 1.319331e-01 5.844763e+00\n",
      "\n",
      " HP Model - Engineered Features\n",
      "                           Model  R² Train  R² Test          MAE         RMSE\n",
      "               Linear Regression  1.000000 1.000000 5.150596e-16 2.741814e-15\n",
      "Polynomial Regression (Degree 2)  0.999981 0.998041 6.231655e-03 2.246454e-01\n",
      "                   Decision Tree  1.000000 0.142854 1.230008e-01 4.699456e+00\n",
      "                   Random Forest  0.978231 0.119252 1.278138e-01 4.763717e+00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "stl_path = \"../processed_data/stl_energy_production_with_engineered_features.csv\"\n",
    "hp_path = \"../processed_data/hp_energy_production_with_engineered_features.csv\"\n",
    "\n",
    "\n",
    "stl_df = pd.read_csv(stl_path)\n",
    "hp_df = pd.read_csv(hp_path)\n",
    "\n",
    "original_features = [\"Water_Flow_m3_s\", \"avgtempC\", \"totalprecipMM\", \"humidity\", \"pressureMB\"]\n",
    "engineered_features = original_features + [\n",
    "    \"WaterFlow_Diff_1d\", \"WaterFlow_Diff_7d\",\n",
    "    \"WaterFlow_3day_avg\", \"WaterFlow_7day_avg\",\n",
    "    \"Temp_Deviation\", \"WaterFlow_Humidity\",\n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"Normalized_Efficiency\", \"Prev_Day_Efficiency\", \"Prev_Week_Efficiency\"\n",
    "]\n",
    "target = \"Efficiency\"\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(df, feature_set, model_name):\n",
    "    X = df[feature_set]\n",
    "    y = df[target]\n",
    "\n",
    "    # Train-Test Split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Polynomial Regression (Degree 2)\": PolynomialFeatures(degree=2),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        if \"Polynomial\" in model_name:\n",
    "            poly = PolynomialFeatures(degree=2)\n",
    "            X_train_poly = poly.fit_transform(X_train)\n",
    "            X_test_poly = poly.transform(X_test)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_poly, y_train)\n",
    "            y_train_pred = model.predict(X_train_poly)\n",
    "            y_test_pred = model.predict(X_test_poly)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate Model\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "        results.append([model_name, r2_train, r2_test, mae, rmse])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Model\", \"R² Train\", \"R² Test\", \"MAE\", \"RMSE\"])\n",
    "\n",
    "# Train & Evaluate on Original Features\n",
    "stl_results_original = train_and_evaluate_model(stl_df, original_features, \"STL Model - Original Features\")\n",
    "hp_results_original = train_and_evaluate_model(hp_df, original_features, \"HP Model - Original Features\")\n",
    "\n",
    "# Train & Evaluate on Engineered Features\n",
    "stl_results_engineered = train_and_evaluate_model(stl_df, engineered_features, \"STL Model - Engineered Features\")\n",
    "hp_results_engineered = train_and_evaluate_model(hp_df, engineered_features, \"HP Model - Engineered Features\")\n",
    "\n",
    "print(\"\\n STL Model - Original Features\")\n",
    "print(stl_results_original.to_string(index=False))\n",
    "\n",
    "print(\"\\n HP Model - Original Features\")\n",
    "print(hp_results_original.to_string(index=False))\n",
    "\n",
    "print(\"\\n STL Model - Engineered Features\")\n",
    "print(stl_results_engineered.to_string(index=False))\n",
    "\n",
    "print(\"\\n HP Model - Engineered Features\")\n",
    "print(hp_results_engineered.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189dd13a-564b-485d-9843-970471755c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2746\n",
      "[LightGBM] [Info] Number of data points in the train set: 8040, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -0.015102\n",
      "\n",
      "STL Advanced Model Results\n",
      "                    Model  R² Train  R² Test          MAE     RMSE\n",
      "                  XGBoost  0.999993 0.117788 1.359856e-01 5.777403\n",
      "                 LightGBM  0.653523 0.052679 1.452606e-01 5.986799\n",
      "                 CatBoost  0.999726 0.117427 1.362831e-01 5.778585\n",
      "        Gradient Boosting  0.999975 0.118079 1.306344e-01 5.776449\n",
      "Support Vector Regression  0.014339 0.000665 1.965852e-01 6.148960\n",
      "      K-Nearest Neighbors  0.450712 0.045111 1.865888e-01 6.010668\n",
      "         Ridge Regression  1.000000 1.000000 5.620185e-07 0.000018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2746\n",
      "[LightGBM] [Info] Number of data points in the train set: 8040, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -0.019524\n",
      "\n",
      "HP Advanced Model Results\n",
      "                    Model  R² Train  R² Test          MAE     RMSE\n",
      "                  XGBoost  0.999995 0.118390 1.381608e-01 4.766050\n",
      "                 LightGBM  0.887040 0.078371 1.482572e-01 4.873020\n",
      "                 CatBoost  0.999432 0.117227 1.422472e-01 4.769190\n",
      "        Gradient Boosting  0.999952 0.136466 1.266584e-01 4.716936\n",
      "Support Vector Regression  0.008317 0.000492 2.245039e-01 5.074735\n",
      "      K-Nearest Neighbors  0.337902 0.033943 2.307047e-01 4.989092\n",
      "         Ridge Regression  1.000000 1.000000 2.411218e-07 0.000005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "advanced_models = {\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, random_state=42, verbosity=0),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=200, random_state=42),\n",
    "    \"CatBoost\": CatBoostRegressor(n_estimators=200, random_state=42, verbose=0),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, random_state=42),\n",
    "    \"Support Vector Regression\": SVR(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"Ridge Regression\": Ridge()\n",
    "}\n",
    "\n",
    "def train_and_evaluate_advanced_models(df, feature_set, dataset_name):\n",
    "    X = df[feature_set]\n",
    "    y = df[\"Efficiency\"]  \n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in advanced_models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate Model\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "        results.append([model_name, r2_train, r2_test, mae, rmse])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"R² Train\", \"R² Test\", \"MAE\", \"RMSE\"])\n",
    "\n",
    "    print(f\"\\n{dataset_name} Advanced Model Results\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    return results_df  \n",
    "\n",
    "\n",
    "stl_results = train_and_evaluate_advanced_models(stl_df, engineered_features, \"STL\")\n",
    "hp_results = train_and_evaluate_advanced_models(hp_df, engineered_features, \"HP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca19e898-ca85-4629-8f32-6b39b7169847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best STL Models Saved:\n",
      "            Model  R² Train  R² Test      MAE     RMSE\n",
      "          XGBoost  0.999993 0.117788 0.135986 5.777403\n",
      "         CatBoost  0.999726 0.117427 0.136283 5.778585\n",
      "Gradient Boosting  0.999975 0.118079 0.130634 5.776449\n",
      "\n",
      " Best HP Models Saved:\n",
      "            Model  R² Train  R² Test      MAE     RMSE\n",
      "          XGBoost  0.999995 0.118390 0.138161 4.766050\n",
      "         CatBoost  0.999432 0.117227 0.142247 4.769190\n",
      "Gradient Boosting  0.999952 0.136466 0.126658 4.716936\n"
     ]
    }
   ],
   "source": [
    "best_stl_models = stl_results[stl_results[\"Model\"].isin([\"XGBoost\", \"CatBoost\", \"Gradient Boosting\"])]\n",
    "best_hp_models = hp_results[hp_results[\"Model\"].isin([\"XGBoost\", \"CatBoost\", \"Gradient Boosting\"])]\n",
    "\n",
    "\n",
    "best_stl_models.to_csv(\"../processed_data/STL_best_models.csv\", index=False)\n",
    "best_hp_models.to_csv(\"../processed_data/HP_best_models.csv\", index=False)\n",
    "\n",
    "print(\"\\n Best STL Models Saved:\")\n",
    "print(best_stl_models.to_string(index=False))\n",
    "\n",
    "print(\"\\n Best HP Models Saved:\")\n",
    "print(best_hp_models.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c4037-3673-4688-a7a9-d4ad117bfd56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (myenv-py311)",
   "language": "python",
   "name": "myenv-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
