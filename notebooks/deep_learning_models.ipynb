{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70de4844-a428-453c-95ba-52661302ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stl_path = \"../processed_data/stl_energy_production_with_engineered_features.csv\"\n",
    "hp_path = \"../processed_data/hp_energy_production_with_engineered_features.csv\"\n",
    "\n",
    "stl_df = pd.read_csv(stl_path)\n",
    "hp_df = pd.read_csv(hp_path)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Water_Flow_m3_s\", \"avgtempC\", \"totalprecipMM\", \"humidity\", \"pressureMB\",\n",
    "    \"WaterFlow_Diff_1d\", \"WaterFlow_Diff_7d\", \"WaterFlow_3day_avg\", \"WaterFlow_7day_avg\",\n",
    "    \"Temp_Deviation\", \"WaterFlow_Humidity\", \"month_sin\", \"month_cos\",\n",
    "    \"Normalized_Efficiency\", \"Prev_Day_Efficiency\", \"Prev_Week_Efficiency\"\n",
    "]\n",
    "target_col = \"Efficiency\"\n",
    "\n",
    "def prepare_data(df):\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values.reshape(-1, 1)\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler_y\n",
    "\n",
    "stl_X_train, stl_X_test, stl_y_train, stl_y_test, stl_scaler_y = prepare_data(stl_df)\n",
    "hp_X_train, hp_X_test, hp_y_train, hp_y_test, hp_scaler_y = prepare_data(hp_df)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "stl_train_loader = DataLoader(TensorDataset(stl_X_train, stl_y_train), batch_size=batch_size, shuffle=True)\n",
    "stl_test_loader = DataLoader(TensorDataset(stl_X_test, stl_y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "hp_train_loader = DataLoader(TensorDataset(hp_X_train, hp_y_train), batch_size=batch_size, shuffle=True)\n",
    "hp_test_loader = DataLoader(TensorDataset(hp_X_test, hp_y_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4641495-0849-49e5-9b70-b3c3ff6fbab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HydropowerEfficiencyNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HydropowerEfficiencyNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HydropowerEfficiencyNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 1)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "input_dim = len(feature_cols)\n",
    "model = HydropowerEfficiencyNN(input_dim)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac3bf3c1-e5c3-4721-8b9d-6111c8bf7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625dc36d-3107-41b4-9935-c2873ddd8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 1.3649\n",
      "Epoch 10/50, Loss: 1.2932\n",
      "Epoch 15/50, Loss: 0.9217\n",
      "Epoch 20/50, Loss: 0.5410\n",
      "Epoch 25/50, Loss: 0.2673\n",
      "Epoch 30/50, Loss: 0.4040\n",
      "Epoch 35/50, Loss: 0.2449\n",
      "Epoch 40/50, Loss: 0.1253\n",
      "Epoch 45/50, Loss: 0.2786\n",
      "Epoch 50/50, Loss: 0.1313\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "\n",
    "train_model(model, stl_train_loader, stl_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364a76b9-82e5-4f77-aa3a-c1be2dc82c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results - R²: 0.9572, MAE: 0.0590, RMSE: 1.2721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, test_loader, scaler_y):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred_batch = model(X_batch).cpu().numpy()\n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(y_pred_batch)\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    y_true = scaler_y.inverse_transform(y_true)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"Evaluation Results - R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "evaluate_model(model, stl_test_loader, stl_scaler_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ae5cb-1f3a-4049-b390-82c2120ee3b9",
   "metadata": {},
   "source": [
    "# Some improvements made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c9b265b-301c-4657-9ca5-f3786402579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "stl_path = \"../processed_data/stl_energy_production_with_engineered_features.csv\"\n",
    "hp_path = \"../processed_data/hp_energy_production_with_engineered_features.csv\"\n",
    "\n",
    "stl_df = pd.read_csv(stl_path)\n",
    "hp_df = pd.read_csv(hp_path)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Water_Flow_m3_s\", \"avgtempC\", \"totalprecipMM\", \"humidity\", \"pressureMB\",\n",
    "    \"WaterFlow_Diff_1d\", \"WaterFlow_Diff_7d\", \"WaterFlow_3day_avg\", \"WaterFlow_7day_avg\",\n",
    "    \"Temp_Deviation\", \"WaterFlow_Humidity\", \"month_sin\", \"month_cos\",\n",
    "    \"Normalized_Efficiency\", \"Prev_Day_Efficiency\", \"Prev_Week_Efficiency\"\n",
    "]\n",
    "target_col = \"Efficiency\"\n",
    "\n",
    "def prepare_data(df):\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values.reshape(-1, 1)\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32),\n",
    "        scaler_y\n",
    "    )\n",
    "\n",
    "stl_X_train, stl_X_test, stl_y_train, stl_y_test, stl_scaler_y = prepare_data(stl_df)\n",
    "hp_X_train, hp_X_test, hp_y_train, hp_y_test, hp_scaler_y = prepare_data(hp_df)\n",
    "\n",
    "batch_size = 64\n",
    "stl_train_loader = DataLoader(TensorDataset(stl_X_train, stl_y_train), batch_size=batch_size, shuffle=True)\n",
    "stl_test_loader = DataLoader(TensorDataset(stl_X_test, stl_y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "hp_train_loader = DataLoader(TensorDataset(hp_X_train, hp_y_train), batch_size=batch_size, shuffle=True)\n",
    "hp_test_loader = DataLoader(TensorDataset(hp_X_test, hp_y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4556d5ba-3760-4700-8407-b85337e5bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydropowerEfficiencyNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HydropowerEfficiencyNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),  # 🔹 Use GELU instead of ReLU\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),  # 🔹 Swish activation function\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(16, 1)  # 🔹 Output Layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "input_dim = len(feature_cols)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = HydropowerEfficiencyNN(input_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d170cd4-5f41-4b33-97ae-55f22cd0d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "633d21f5-9542-488a-bc2e-caf9757808f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 2.4254\n",
      "Epoch 10/50, Loss: 1.4886\n",
      "Epoch 15/50, Loss: 1.3925\n",
      "Epoch 20/50, Loss: 1.1160\n",
      "Epoch 25/50, Loss: 0.8589\n",
      "Epoch 30/50, Loss: 0.6407\n",
      "Epoch 35/50, Loss: 0.6818\n",
      "Epoch 40/50, Loss: 0.5180\n",
      "Epoch 45/50, Loss: 0.6824\n",
      "Epoch 50/50, Loss: 0.2490\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "train_model(model, stl_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704ec47a-71f6-43c7-a579-d9283853de44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results - R²: 0.9912, MAE: 0.0568, RMSE: 0.5758\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, scaler_y):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred_batch = model(X_batch).cpu().numpy()\n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(y_pred_batch)\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    y_true = scaler_y.inverse_transform(y_true)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"\\nEvaluation Results - R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "evaluate_model(model, stl_test_loader, stl_scaler_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d7915-c2d8-4785-a089-cc34c9c9a5fb",
   "metadata": {},
   "source": [
    "# RNN/LSTM/GRU\n",
    "Best results after several tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af43bd13-f999-4988-b0f5-ef46cf102864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, feature_cols, target_col, seq_length=14):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        X.append(df[feature_cols].iloc[i:i+seq_length].values)  \n",
    "        y.append(df[target_col].iloc[i+seq_length])  \n",
    "    \n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "stl_X, stl_y = create_sequences(stl_df, feature_cols, target_col, seq_length=14)\n",
    "hp_X, hp_y = create_sequences(hp_df, feature_cols, target_col, seq_length=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7440aee-1764-4b21-9fb3-f711a6de08d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([8028, 14, 16]), Test shape: torch.Size([2008, 14, 16])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stl_X, stl_y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = map(\n",
    "    lambda x: torch.tensor(x, dtype=torch.float32).to(device),\n",
    "    (X_train, X_test, y_train, y_test)\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "13312675-b17f-4c18-b754-2919add75661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, model_type=\"RNN\", dropout_rate=0.3):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if model_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  \n",
    "        return self.fc(out[:, -1, :])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f985066b-4a9e-4f3b-98f3-310fa4ba74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=75):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step(epoch_loss)  \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f19547c-c2f6-4d99-9892-b1fc1f77b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler_y):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)  \n",
    "            y_pred_batch = model(X_batch).cpu().numpy()  \n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(y_pred_batch)\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    y_true = scaler_y.inverse_transform(y_true)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"Evaluation Results - R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "    return r2, mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f98977aa-5650-4e16-8374-c7168a4c2a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN...\n",
      "Epoch 5/75, Loss: 12.7177\n",
      "Epoch 10/75, Loss: 11.2720\n",
      "Epoch 15/75, Loss: 10.2720\n",
      "Epoch 20/75, Loss: 9.6341\n",
      "Epoch 25/75, Loss: 9.2514\n",
      "Epoch 30/75, Loss: 9.3310\n",
      "Epoch 35/75, Loss: 9.0778\n",
      "Epoch 40/75, Loss: 9.1271\n",
      "Epoch 45/75, Loss: 9.1025\n",
      "Epoch 50/75, Loss: 10.0806\n",
      "Epoch 55/75, Loss: 9.3694\n",
      "Epoch 60/75, Loss: 9.0561\n",
      "Epoch 65/75, Loss: 9.0619\n",
      "Epoch 70/75, Loss: 8.9237\n",
      "Epoch 75/75, Loss: 8.8889\n",
      "Evaluation Results - R²: 0.0525, MAE: 0.5077, RMSE: 16.5767\n",
      "RNN - R²: 0.0525, MAE: 0.5077, RMSE: 16.5767\n",
      "\n",
      "Training LSTM...\n",
      "Epoch 5/75, Loss: 12.0572\n",
      "Epoch 10/75, Loss: 9.7224\n",
      "Epoch 15/75, Loss: 9.6891\n",
      "Epoch 20/75, Loss: 9.4526\n",
      "Epoch 25/75, Loss: 8.9858\n",
      "Epoch 30/75, Loss: 8.9516\n",
      "Epoch 35/75, Loss: 8.7706\n",
      "Epoch 40/75, Loss: 8.7575\n",
      "Epoch 45/75, Loss: 8.3188\n",
      "Epoch 50/75, Loss: 8.0623\n",
      "Epoch 55/75, Loss: 8.2088\n",
      "Epoch 60/75, Loss: 8.5858\n",
      "Epoch 65/75, Loss: 7.6321\n",
      "Epoch 70/75, Loss: 7.6907\n",
      "Epoch 75/75, Loss: 9.0339\n",
      "Evaluation Results - R²: 0.0664, MAE: 0.5152, RMSE: 16.4548\n",
      "LSTM - R²: 0.0664, MAE: 0.5152, RMSE: 16.4548\n",
      "\n",
      "Training GRU...\n",
      "Epoch 5/75, Loss: 12.4888\n",
      "Epoch 10/75, Loss: 10.9421\n",
      "Epoch 15/75, Loss: 9.7681\n",
      "Epoch 20/75, Loss: 9.6447\n",
      "Epoch 25/75, Loss: 9.5244\n",
      "Epoch 30/75, Loss: 9.2182\n",
      "Epoch 35/75, Loss: 9.2973\n",
      "Epoch 40/75, Loss: 9.1022\n",
      "Epoch 45/75, Loss: 9.0872\n",
      "Epoch 50/75, Loss: 9.0592\n",
      "Epoch 55/75, Loss: 9.0156\n",
      "Epoch 60/75, Loss: 8.8763\n",
      "Epoch 65/75, Loss: 8.8025\n",
      "Epoch 70/75, Loss: 8.8850\n",
      "Epoch 75/75, Loss: 8.9570\n",
      "Evaluation Results - R²: 0.0587, MAE: 0.5026, RMSE: 16.5229\n",
      "GRU - R²: 0.0587, MAE: 0.5026, RMSE: 16.5229\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 128 \n",
    "\n",
    "for model_type in [\"RNN\", \"LSTM\", \"GRU\"]:\n",
    "    print(f\"\\nTraining {model_type}...\")\n",
    "    model = RecurrentModel(len(feature_cols), hidden_dim, model_type=model_type)\n",
    "    trained_model = train_model(model, train_loader, test_loader)\n",
    "    r2, mae, rmse = evaluate_model(trained_model, test_loader, stl_scaler_y)\n",
    "    print(f\"{model_type} - R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (myenv-py311)",
   "language": "python",
   "name": "myenv-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
